# Before vs After: Categorical Encoding

## ğŸ” Why This Matters

Machine learning models require **numeric input**. Text categories like "Male", "Female", "DSL" cannot be processed by algorithms.

---

## ğŸ“Š Before Encoding (Current State)

### Sample Row from `data/processed/train.csv`:
```csv
gender,SeniorCitizen,Partner,Dependents,tenure,PhoneService,MultipleLines,InternetService,...,Churn
Male,0,No,No,26,Yes,No,DSL,...,0
```

### Data Types:
- **Text/Categorical** ğŸ”´: `gender`, `Partner`, `Dependents`, `PhoneService`, `MultipleLines`, `InternetService`, `OnlineSecurity`, `OnlineBackup`, `DeviceProtection`, `TechSupport`, `StreamingTV`, `StreamingMovies`, `Contract`, `PaperlessBilling`, `PaymentMethod`, `tenure_bin`
- **Numeric** âœ…: `SeniorCitizen`, `tenure`, `MonthlyCharges`, `TotalCharges`, `numAdminTickets`, `numTechTickets`, `avg_monthly_charges`
- **Target** âœ…: `Churn` (0/1)

### Problem:
âŒ Models like Random Forest, XGBoost, Neural Networks **cannot** process text  
âŒ Will throw errors or silently fail  
âŒ Need to convert "Male" â†’ numeric representation

---

## ğŸ¯ After Encoding (With One-Hot Encoding)

### What One-Hot Encoding Does:

**Original Column:**
```
gender
------
Male
Female
Male
```

**After One-Hot Encoding:**
```
gender_Female  gender_Male
-----------    -----------
0              1
1              0
0              1
```

Each category becomes a **binary column** (0 or 1).

---

## ğŸ”¢ Expected Output After Encoding

### From ~24 columns â†’ ~60-70 columns

#### Original Categories â†’ One-Hot Columns:

1. **gender** (2 values) â†’ `gender_Female`, `gender_Male`
2. **Partner** (2 values) â†’ `Partner_No`, `Partner_Yes`
3. **Dependents** (2 values) â†’ `Dependents_No`, `Dependents_Yes`
4. **PhoneService** (2 values) â†’ `PhoneService_No`, `PhoneService_Yes`
5. **MultipleLines** (3 values) â†’ `MultipleLines_No`, `MultipleLines_No phone service`, `MultipleLines_Yes`
6. **InternetService** (3 values) â†’ `InternetService_DSL`, `InternetService_Fiber optic`, `InternetService_No`
7. **OnlineSecurity** (3 values) â†’ `OnlineSecurity_No`, `OnlineSecurity_No internet service`, `OnlineSecurity_Yes`
8. **OnlineBackup** (3 values) â†’ `OnlineBackup_No`, `OnlineBackup_No internet service`, `OnlineBackup_Yes`
9. **DeviceProtection** (3 values) â†’ Similar to above
10. **TechSupport** (3 values) â†’ Similar to above
11. **StreamingTV** (3 values) â†’ Similar to above
12. **StreamingMovies** (3 values) â†’ Similar to above
13. **Contract** (3 values) â†’ `Contract_Month-to-month`, `Contract_One year`, `Contract_Two year`
14. **PaperlessBilling** (2 values) â†’ `PaperlessBilling_No`, `PaperlessBilling_Yes`
15. **PaymentMethod** (4 values) â†’ `PaymentMethod_Bank transfer (automatic)`, `PaymentMethod_Credit card (automatic)`, `PaymentMethod_Electronic check`, `PaymentMethod_Mailed check`
16. **tenure_bin** (4 values) â†’ `tenure_bin_0-1yr`, `tenure_bin_1-2yr`, `tenure_bin_2-4yr`, `tenure_bin_4+yr`

**Numeric columns remain unchanged:**
- `SeniorCitizen`, `tenure`, `MonthlyCharges`, `TotalCharges`, `numAdminTickets`, `numTechTickets`, `avg_monthly_charges`

**Target remains:**
- `Churn` (0/1)

---

## ğŸ†š Side-by-Side Comparison

### Before (24 columns):
```
gender | SeniorCitizen | Partner | tenure | Contract      | MonthlyCharges | Churn
Male   | 0             | No      | 26     | One year      | 59.45          | 0
Male   | 1             | Yes     | 72     | Two year      | 116.05         | 0
Female | 0             | No      | 1      | Month-to-month| 29.85          | 1
```
âŒ Models cannot use "Male", "No", "One year" directly

### After (~60-70 columns):
```
gender_Male | gender_Female | SeniorCitizen | Partner_Yes | Partner_No | tenure | Contract_One year | Contract_Two year | Contract_Month-to-month | MonthlyCharges | Churn
1           | 0             | 0             | 0           | 1          | 26     | 1                 | 0                 | 0                       | 59.45          | 0
1           | 0             | 1             | 1           | 0          | 72     | 0                 | 1                 | 0                       | 116.05         | 0
0           | 1             | 0             | 0           | 1          | 1      | 0                 | 0                 | 1                       | 29.85          | 1
```
âœ… All columns are now numeric (0s and 1s, plus continuous values)

---

## ğŸ› ï¸ What We Added to the Pipeline

### 1. Created `src/data/encoding.py`

**Main Function:**
```python
def encode_categorical_features(df, encoding_type='onehot'):
    """
    Encode categorical features to numeric format.
    
    - encoding_type='onehot': Creates binary columns (recommended)
    - encoding_type='label': Converts to integers (0,1,2,...)
    """
    # Automatically detects columns with text/object dtype
    # Uses pd.get_dummies() for one-hot encoding
    # Returns DataFrame with all numeric columns
```

### 2. Updated `src/pipeline.py`

**Added Step 7 (between feature engineering and splitting):**
```python
# Step 7: Encode categorical features
print("\n[7/8] Encoding categorical features...")
df = encode_categorical_features(df, encoding_type='onehot')
print("âœ“ Categorical features encoded")
```

**Pipeline Flow (8 steps now):**
1. Load raw data
2. Convert data types (TotalCharges string â†’ numeric)
3. Clean data (missing values)
4. Validate data
5. Encode target (Churn: Yes/No â†’ 1/0)
6. Engineer features (tenure_bin, avg_monthly_charges)
7. **ğŸ†• Encode categorical features (text â†’ numeric)** â† NEW!
8. Split data (train/val/test)

---

## ğŸ“ˆ Benefits of One-Hot Encoding

### âœ… Advantages:
1. **No ordinal assumption**: "Male" vs "Female" are not ordered (0 vs 1 implies order)
2. **Works with all algorithms**: Random Forest, XGBoost, Neural Networks, etc.
3. **Interpretable**: Each binary column has clear meaning
4. **Captures all information**: No information loss

### âš ï¸ Considerations:
1. **Increases dimensionality**: 24 columns â†’ 60-70 columns
2. **Sparse data**: Many 0s, only one 1 per category
3. **Memory usage**: More columns = more memory

### Alternative: Label Encoding
If you prefer fewer columns, change to `encoding_type='label'`:
```python
# This would convert:
# "Male" â†’ 0, "Female" â†’ 1
# "Month-to-month" â†’ 0, "One year" â†’ 1, "Two year" â†’ 2
```
**Downside**: Implies ordering (Month-to-month < One year < Two year), which may not be meaningful.

---

## ğŸš€ Running the Updated Pipeline

```bash
# Clean old processed data
rm -rf data/processed/*.csv

# Run updated pipeline with encoding
python src/pipeline.py

# Or use Makefile
make data
```

### Expected Output:
```
Starting Data Processing Pipeline
============================================================

[1/8] Loading raw data...
âœ“ Loaded 7043 rows

[2/8] Converting data types...
âœ“ Data types converted

[3/8] Cleaning data...
âœ“ Cleaned data: 7032 rows remaining

[4/8] Validating data...
âœ“ Data validated

[5/8] Encoding target variable...
âœ“ Target encoded

[6/8] Engineering features...
âœ“ Features engineered

[7/8] Encoding categorical features...
  Encoding 16 categorical columns...
  âœ“ One-hot encoding complete: 67 total columns
âœ“ Categorical features encoded

[8/8] Splitting data into train/val/test sets...
Train size: 4508
Validation size: 1128
Test size: 1410
Saved splits to data/processed

============================================================
Data Pipeline Complete!
============================================================

Processed data saved to: data/processed/
  - train.csv: 4508 samples
  - val.csv: 1128 samples
  - test.csv: 1410 samples

Ready for model training!
```

---

## âœ… Verification

After running, check the processed data:

```bash
# Check number of columns (should be ~60-70)
head -1 data/processed/train.csv | awk -F',' '{print NF}'

# Check first few column names
head -1 data/processed/train.csv | tr ',' '\n' | head -20

# Verify all values are numeric (no text)
head -5 data/processed/train.csv
```

**All values should now be numbers!** ğŸ‰

---

## ğŸ“ Key Takeaway

**Before**: Text categories â†’ âŒ Models fail  
**After**: All numeric â†’ âœ… Models work  

One-hot encoding is the **bridge** between raw data and machine learning models!

