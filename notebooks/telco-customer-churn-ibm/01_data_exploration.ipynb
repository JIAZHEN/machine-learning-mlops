{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Telco Customer Churn - Exploratory Data Analysis\n",
        "\n",
        "**Objective:** Understand the customer churn dataset, identify patterns, and prepare for modeling.\n",
        "\n",
        "**Data Source:** `data/raw/customer_churn_dataset.xlsx`\n",
        "\n",
        "**Mindset:** Trust nothing, verify everything. Every analysis should answer: *\"How does this help me build a better model?\"*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "\n",
        "# Configure visualization\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "%matplotlib inline\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the Excel file from raw data\n",
        "data_path = '../../data/raw/customer_churn_dataset.xlsx'\n",
        "\n",
        "print(f\"Loading data from: {data_path}\")\n",
        "df = pd.read_excel(data_path)\n",
        "\n",
        "print(f\"\\nâœ“ Dataset loaded successfully!\")\n",
        "print(f\"Shape: {df.shape}\")\n",
        "print(f\"Rows: {df.shape[0]:,}\")\n",
        "print(f\"Columns: {df.shape[1]}\")\n",
        "\n",
        "# Display first few rows\n",
        "print(\"\\n=== First 5 Rows ===\")\n",
        "display(df.head())\n",
        "\n",
        "# Display last few rows\n",
        "print(\"\\n=== Last 5 Rows ===\")\n",
        "display(df.tail())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Overview\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic info\n",
        "print(\"=== Data Types ===\")\n",
        "df.info()\n",
        "\n",
        "print(\"\\n=== Column Names ===\")\n",
        "print(df.columns.tolist())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Statistical summary\n",
        "df.describe()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Categorical summary\n",
        "print(\"=== Categorical Features Summary ===\")\n",
        "df.describe(include='object')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Check for Duplicates\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for duplicate rows\n",
        "duplicates = df.duplicated().sum()\n",
        "print(f\"Duplicate rows: {duplicates}\")\n",
        "\n",
        "if duplicates > 0:\n",
        "    print(f\"âš ï¸ Warning: {duplicates} duplicate rows found ({duplicates/len(df)*100:.2f}%)\")\n",
        "    print(\"\\nSample duplicates:\")\n",
        "    display(df[df.duplicated(keep=False)].head(10))\n",
        "else:\n",
        "    print(\"âœ“ No duplicate rows found\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Quality - Missing Values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Missing values\n",
        "missing = pd.DataFrame({\n",
        "    'Missing Count': df.isnull().sum(),\n",
        "    'Percentage': (df.isnull().sum() / len(df) * 100).round(2)\n",
        "})\n",
        "missing = missing[missing['Missing Count'] > 0].sort_values('Missing Count', ascending=False)\n",
        "\n",
        "if len(missing) > 0:\n",
        "    print(\"âš ï¸ Missing Values Found:\")\n",
        "    print(missing)\n",
        "    \n",
        "    # Visualize\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    missing['Percentage'].plot(kind='barh')\n",
        "    plt.xlabel('Percentage Missing (%)')\n",
        "    plt.title('Missing Values by Feature')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"âœ“ No missing values found\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Target Variable Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify target column (typically 'Churn', 'Churn Label', etc.)\n",
        "churn_cols = [col for col in df.columns if 'churn' in col.lower()]\n",
        "print(f\"Potential churn columns: {churn_cols}\")\n",
        "\n",
        "# Set target column - adjust based on your data\n",
        "target_col = churn_cols[0] if churn_cols else 'Churn'\n",
        "\n",
        "if target_col in df.columns:\n",
        "    print(f\"\\n=== Target Variable: {target_col} ===\")\n",
        "    print(df[target_col].value_counts())\n",
        "    print(f\"\\nChurn Rate: {(df[target_col].value_counts(normalize=True) * 100).round(2)}\")\n",
        "    \n",
        "    # Visualize\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    # Count plot\n",
        "    df[target_col].value_counts().plot(kind='bar', ax=axes[0], color=['#2ecc71', '#e74c3c'])\n",
        "    axes[0].set_title('Churn Distribution (Count)', fontsize=14, fontweight='bold')\n",
        "    axes[0].set_xlabel(target_col)\n",
        "    axes[0].set_ylabel('Count')\n",
        "    axes[0].tick_params(axis='x', rotation=0)\n",
        "    \n",
        "    # Percentage plot\n",
        "    df[target_col].value_counts(normalize=True).plot(kind='bar', ax=axes[1], color=['#2ecc71', '#e74c3c'])\n",
        "    axes[1].set_title('Churn Distribution (Percentage)', fontsize=14, fontweight='bold')\n",
        "    axes[1].set_xlabel(target_col)\n",
        "    axes[1].set_ylabel('Proportion')\n",
        "    axes[1].tick_params(axis='x', rotation=0)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(f\"âš ï¸ Warning: '{target_col}' column not found. Please identify the target variable.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Class Balance Check\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check class balance\n",
        "if target_col in df.columns:\n",
        "    value_counts = df[target_col].value_counts()\n",
        "    percentages = df[target_col].value_counts(normalize=True) * 100\n",
        "    \n",
        "    print(\"=== Class Distribution ===\")\n",
        "    for val, count in value_counts.items():\n",
        "        pct = percentages[val]\n",
        "        print(f\"{val}: {count:,} ({pct:.2f}%)\")\n",
        "    \n",
        "    # Calculate imbalance ratio\n",
        "    imbalance_ratio = value_counts.max() / value_counts.min()\n",
        "    print(f\"\\nImbalance Ratio: {imbalance_ratio:.2f}:1\")\n",
        "    \n",
        "    if imbalance_ratio > 3:\n",
        "        print(\"âš ï¸ Significant class imbalance detected!\")\n",
        "        print(\"Consider: SMOTE, class weights, or stratified sampling\")\n",
        "    else:\n",
        "        print(\"âœ“ Classes are reasonably balanced\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Numerical Features Distribution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get numerical columns\n",
        "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "print(f\"Numerical columns ({len(numeric_cols)}): {numeric_cols}\")\n",
        "\n",
        "# Plot distributions\n",
        "if len(numeric_cols) > 0:\n",
        "    n_cols = 3\n",
        "    n_rows = (len(numeric_cols) + n_cols - 1) // n_cols\n",
        "    \n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
        "    axes = axes.flatten() if n_rows > 1 else [axes] if n_cols == 1 else axes\n",
        "    \n",
        "    for idx, col in enumerate(numeric_cols):\n",
        "        axes[idx].hist(df[col].dropna(), bins=30, edgecolor='black', alpha=0.7)\n",
        "        axes[idx].set_title(f'{col}', fontweight='bold')\n",
        "        axes[idx].set_xlabel(col)\n",
        "        axes[idx].set_ylabel('Frequency')\n",
        "        axes[idx].grid(alpha=0.3)\n",
        "    \n",
        "    # Hide unused subplots\n",
        "    for idx in range(len(numeric_cols), len(axes)):\n",
        "        axes[idx].set_visible(False)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1 Statistical Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detailed statistics for numerical features\n",
        "if len(numeric_cols) > 0:\n",
        "    print(\"=== Numerical Features Statistics ===\\n\")\n",
        "    \n",
        "    stats_df = df[numeric_cols].agg(['count', 'mean', 'median', 'std', 'min', 'max', 'skew']).T\n",
        "    stats_df = stats_df.round(2)\n",
        "    display(stats_df)\n",
        "    \n",
        "    # Identify skewed features\n",
        "    skewed_features = stats_df[abs(stats_df['skew']) > 1].index.tolist()\n",
        "    if skewed_features:\n",
        "        print(f\"\\nâš ï¸ Highly skewed features (|skew| > 1): {skewed_features}\")\n",
        "        print(\"Consider: Log transformation, Box-Cox, or robust scaling\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Categorical Features Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get categorical columns\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "# Remove ID columns if any\n",
        "categorical_cols = [col for col in categorical_cols if 'id' not in col.lower()]\n",
        "\n",
        "print(f\"Categorical columns ({len(categorical_cols)}): {categorical_cols}\")\n",
        "\n",
        "# Show unique values for each categorical column\n",
        "for col in categorical_cols[:10]:  # Limit to first 10\n",
        "    unique_count = df[col].nunique()\n",
        "    print(f\"\\n{col}: {unique_count} unique values\")\n",
        "    print(df[col].value_counts().head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Correlation Analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.1 Cardinality Check\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check cardinality for categorical features\n",
        "if len(categorical_cols) > 0:\n",
        "    print(\"=== Categorical Features Cardinality ===\\n\")\n",
        "    \n",
        "    cardinality_df = pd.DataFrame({\n",
        "        'Feature': categorical_cols,\n",
        "        'Unique Values': [df[col].nunique() for col in categorical_cols],\n",
        "        'Sample Values': [', '.join(map(str, df[col].unique()[:3])) + '...' for col in categorical_cols]\n",
        "    }).sort_values('Unique Values', ascending=False)\n",
        "    \n",
        "    display(cardinality_df)\n",
        "    \n",
        "    # Flag high cardinality features\n",
        "    high_card = cardinality_df[cardinality_df['Unique Values'] > 50]['Feature'].tolist()\n",
        "    if high_card:\n",
        "        print(f\"\\nâš ï¸ High cardinality features (>50 unique): {high_card}\")\n",
        "        print(\"Consider: Target encoding, frequency encoding, or grouping rare categories\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Correlation matrix for numerical features\n",
        "if len(numeric_cols) > 1:\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    correlation_matrix = df[numeric_cols].corr()\n",
        "    \n",
        "    sns.heatmap(correlation_matrix, \n",
        "                annot=True, \n",
        "                fmt='.2f', \n",
        "                cmap='coolwarm', \n",
        "                center=0, \n",
        "                square=True,\n",
        "                linewidths=0.5)\n",
        "    plt.title('Feature Correlation Matrix', fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Find highly correlated features (>0.8)\n",
        "    high_corr = []\n",
        "    for i in range(len(correlation_matrix.columns)):\n",
        "        for j in range(i+1, len(correlation_matrix.columns)):\n",
        "            if abs(correlation_matrix.iloc[i, j]) > 0.8:\n",
        "                high_corr.append((correlation_matrix.columns[i], \n",
        "                                correlation_matrix.columns[j], \n",
        "                                correlation_matrix.iloc[i, j]))\n",
        "    \n",
        "    if high_corr:\n",
        "        print(\"\\nâš ï¸ Highly Correlated Features (|r| > 0.8):\")\n",
        "        for feat1, feat2, corr in high_corr:\n",
        "            print(f\"  {feat1} â†” {feat2}: {corr:.3f}\")\n",
        "    else:\n",
        "        print(\"\\nâœ“ No highly correlated features found\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Churn vs Numerical Features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.1 Top Correlations with Target\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# If target is numeric, show correlations with target\n",
        "if target_col in df.columns and len(numeric_cols) > 1:\n",
        "    # Try to encode target if it's categorical\n",
        "    target_encoded = df[target_col].copy()\n",
        "    if df[target_col].dtype == 'object':\n",
        "        # Simple encoding: Yes/No -> 1/0, or use factorize\n",
        "        if set(df[target_col].unique()).issubset({'Yes', 'No'}):\n",
        "            target_encoded = (df[target_col] == 'Yes').astype(int)\n",
        "        else:\n",
        "            target_encoded = pd.factorize(df[target_col])[0]\n",
        "    \n",
        "    # Calculate correlations\n",
        "    correlations_with_target = df[numeric_cols].corrwith(pd.Series(target_encoded, index=df.index))\n",
        "    correlations_with_target = correlations_with_target.abs().sort_values(ascending=False)\n",
        "    \n",
        "    print(\"=== Features Most Correlated with Target ===\\n\")\n",
        "    print(correlations_with_target.head(10))\n",
        "    \n",
        "    # Visualize\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    correlations_with_target.head(15).plot(kind='barh', color='skyblue')\n",
        "    plt.xlabel('Absolute Correlation')\n",
        "    plt.title('Top 15 Features by Correlation with Target', fontweight='bold')\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Box plots: numerical features vs churn\n",
        "if target_col in df.columns and len(numeric_cols) > 0:\n",
        "    n_features = min(6, len(numeric_cols))  # Show top 6\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    for idx, col in enumerate(numeric_cols[:n_features]):\n",
        "        df.boxplot(column=col, by=target_col, ax=axes[idx])\n",
        "        axes[idx].set_title(f'{col} vs {target_col}')\n",
        "        axes[idx].set_xlabel(target_col)\n",
        "        axes[idx].set_ylabel(col)\n",
        "        plt.sca(axes[idx])\n",
        "        plt.xticks(rotation=0)\n",
        "    \n",
        "    plt.suptitle('Numerical Features vs Churn', fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Churn vs Categorical Features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Churn rate by categorical features\n",
        "if target_col in df.columns and len(categorical_cols) > 0:\n",
        "    n_features = min(6, len(categorical_cols))\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    for idx, col in enumerate(categorical_cols[:n_features]):\n",
        "        if df[col].nunique() <= 10:  # Only plot if not too many categories\n",
        "            churn_rate = df.groupby(col)[target_col].apply(\n",
        "                lambda x: (x == 'Yes').sum() / len(x) if 'Yes' in x.values else x.mean()\n",
        "            ).sort_values(ascending=False)\n",
        "            \n",
        "            churn_rate.plot(kind='bar', ax=axes[idx], color='coral')\n",
        "            axes[idx].set_title(f'Churn Rate by {col}')\n",
        "            axes[idx].set_xlabel(col)\n",
        "            axes[idx].set_ylabel('Churn Rate')\n",
        "            axes[idx].tick_params(axis='x', rotation=45)\n",
        "            axes[idx].grid(alpha=0.3, axis='y')\n",
        "    \n",
        "    plt.suptitle('Churn Rate by Categorical Features', fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.1 Statistical Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare means by target class\n",
        "if target_col in df.columns and len(numeric_cols) > 0:\n",
        "    print(\"=== Mean Values by Target Class ===\\n\")\n",
        "    \n",
        "    comparison = df.groupby(target_col)[numeric_cols].mean().T\n",
        "    comparison['Difference'] = comparison.iloc[:, 1] - comparison.iloc[:, 0]\n",
        "    comparison['Pct_Diff'] = (comparison['Difference'] / comparison.iloc[:, 0] * 100).round(2)\n",
        "    comparison = comparison.reindex(comparison['Pct_Diff'].abs().sort_values(ascending=False).index)\n",
        "    \n",
        "    display(comparison.head(10))\n",
        "    \n",
        "    print(\"\\nðŸ’¡ Features with largest mean differences may be good predictors\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 10.1 Outlier Statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate outliers using IQR method\n",
        "if len(numeric_cols) > 0:\n",
        "    print(\"=== Outlier Count (IQR Method) ===\\n\")\n",
        "    \n",
        "    outlier_summary = []\n",
        "    for col in numeric_cols:\n",
        "        Q1 = df[col].quantile(0.25)\n",
        "        Q3 = df[col].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "        \n",
        "        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
        "        outlier_count = len(outliers)\n",
        "        outlier_pct = (outlier_count / len(df)) * 100\n",
        "        \n",
        "        outlier_summary.append({\n",
        "            'Feature': col,\n",
        "            'Outlier Count': outlier_count,\n",
        "            'Percentage': f\"{outlier_pct:.2f}%\",\n",
        "            'Lower Bound': f\"{lower_bound:.2f}\",\n",
        "            'Upper Bound': f\"{upper_bound:.2f}\"\n",
        "        })\n",
        "    \n",
        "    outlier_df = pd.DataFrame(outlier_summary).sort_values('Outlier Count', ascending=False)\n",
        "    display(outlier_df)\n",
        "    \n",
        "    high_outliers = outlier_df[outlier_df['Outlier Count'] > len(df) * 0.05]['Feature'].tolist()\n",
        "    if high_outliers:\n",
        "        print(f\"\\nâš ï¸ Features with >5% outliers: {high_outliers}\")\n",
        "        print(\"Consider: Capping (winsorization), log transformation, or investigation\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Outlier Detection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Box plots for outlier detection\n",
        "if len(numeric_cols) > 0:\n",
        "    n_features = min(6, len(numeric_cols))\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    for idx, col in enumerate(numeric_cols[:n_features]):\n",
        "        axes[idx].boxplot(df[col].dropna())\n",
        "        axes[idx].set_title(f'{col}')\n",
        "        axes[idx].set_ylabel(col)\n",
        "        axes[idx].grid(alpha=0.3)\n",
        "    \n",
        "    plt.suptitle('Outlier Detection via Box Plots', fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Key Findings Summary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9.1 Statistical Significance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Chi-square test for categorical features\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "if target_col in df.columns and len(categorical_cols) > 0:\n",
        "    print(\"=== Chi-Square Test: Categorical Features vs Target ===\\n\")\n",
        "    \n",
        "    chi_square_results = []\n",
        "    for col in categorical_cols[:10]:  # Limit to first 10\n",
        "        if df[col].nunique() <= 20:  # Only test if reasonable number of categories\n",
        "            contingency_table = pd.crosstab(df[col], df[target_col])\n",
        "            chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
        "            \n",
        "            chi_square_results.append({\n",
        "                'Feature': col,\n",
        "                'Chi2': chi2,\n",
        "                'P-Value': p_value,\n",
        "                'Significant': 'âœ“' if p_value < 0.05 else 'âœ—'\n",
        "            })\n",
        "    \n",
        "    if chi_square_results:\n",
        "        chi_df = pd.DataFrame(chi_square_results).sort_values('P-Value')\n",
        "        display(chi_df)\n",
        "        \n",
        "        significant_features = chi_df[chi_df['P-Value'] < 0.05]['Feature'].tolist()\n",
        "        print(f\"\\nðŸ’¡ Statistically significant features (p < 0.05): {len(significant_features)}\")\n",
        "        print(f\"   {significant_features}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ðŸ“Š Summary & Action Plan\n",
        "\n",
        "Fill in after running all cells above:\n",
        "\n",
        "**âœ… Data Quality:**\n",
        "- Total rows: ___\n",
        "- Missing values: ___\n",
        "- Duplicates: ___\n",
        "\n",
        "**ðŸŽ¯ Target Variable:**\n",
        "- Distribution: ___\n",
        "- Imbalance ratio: ___:1\n",
        "- Baseline accuracy: ___%\n",
        "\n",
        "**ðŸ“Š Top 5 Predictive Features:**\n",
        "1. ___\n",
        "2. ___\n",
        "3. ___\n",
        "4. ___\n",
        "5. ___\n",
        "\n",
        "**ðŸ”§ Preprocessing Plan:**\n",
        "- Missing values: ___\n",
        "- Outliers: ___\n",
        "- Encoding: ___\n",
        "- Scaling: ___\n",
        "- Class imbalance: ___\n",
        "\n",
        "**ðŸš« Features to Drop:**\n",
        "- ___ (reason: ___)\n",
        "\n",
        "**ðŸš€ Next Steps:**\n",
        "1. [ ] Run data pipeline: `python src/pipeline.py`\n",
        "2. [ ] Train baseline model\n",
        "3. [ ] Track in MLflow\n",
        "\n",
        "**ðŸ’¡ Key Insights:**\n",
        "- ___\n",
        "- ___\n",
        "- ___\n",
        "\n",
        "**Data Quality:**\n",
        "- [ ] Missing values: [describe]\n",
        "- [ ] Duplicates: [check]\n",
        "- [ ] Data types: [correct/needs fixing]\n",
        "\n",
        "**Target Variable:**\n",
        "- [ ] Class imbalance: [percentage]\n",
        "- [ ] Action needed: [balancing strategy if needed]\n",
        "\n",
        "**Features:**\n",
        "- [ ] Most correlated with churn: [list top 3-5]\n",
        "- [ ] Highly correlated features: [list pairs to consider removing]\n",
        "- [ ] Outliers: [significant outliers found in which features?]\n",
        "\n",
        "**Next Steps:**\n",
        "1. [ ] Handle missing values (strategy: ___)\n",
        "2. [ ] Feature engineering ideas: [list]\n",
        "3. [ ] Features to drop: [list with reasons]\n",
        "4. [ ] Encoding strategy: [one-hot, label, etc.]\n",
        "5. [ ] Scaling needed: [yes/no, which features]\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
